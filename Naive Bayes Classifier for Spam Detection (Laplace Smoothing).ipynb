{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:05:32.664122Z",
     "start_time": "2019-09-17T02:05:32.626709Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def data_loader():\n",
    "    # load data\n",
    "    with open('train','r') as file:\n",
    "        train_data = file.read().split('\\n')[:-1]\n",
    "    with open('test','r') as file:\n",
    "        test_data = file.read().split('\\n')[:-1]\n",
    "    return train_data, test_data\n",
    "\n",
    "def parser(datum):\n",
    "    # extract labels and words\n",
    "    email_addr, label, words = datum.split(' ',2)\n",
    "    words = words.split()\n",
    "    # transform words into dictionary\n",
    "    word_dict = dict(zip([words[i] for i in range(0, len(words), 2)], [int(words[i+1]) for i in range(0, len(words), 2)]))\n",
    "    # transform label into 0, 1\n",
    "    if label == 'ham':\n",
    "        label = 0\n",
    "    elif label == 'spam':\n",
    "        label = 1\n",
    "    else: \n",
    "        raise ValueError\n",
    "    return label, word_dict\n",
    "\n",
    "def data_preprocessing(train_data, test_data):\n",
    "    y_train = np.zeros(len(train_data))\n",
    "    y_test = np.zeros(len(test_data))\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    for i, datum in enumerate(train_data):\n",
    "        label, word_dict = parser(datum)\n",
    "        y_train[i] = label\n",
    "        x_train.append(word_dict)\n",
    "    for i, datum in enumerate(test_data):\n",
    "        label, word_dict = parser(datum)\n",
    "        y_test[i] = label\n",
    "        x_test.append(word_dict)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_pmf_y(y_train):\n",
    "    # compte distribution P(y=1), P(y=0)\n",
    "    p_y1=sum(y_train)/len(y_train) #spam\n",
    "    p_y0=1-p_y1  #ham\n",
    "    return p_y1, p_y0\n",
    "\n",
    "def m_estimation_conditional_probability(x_train_frt, y_train, num_vocab, m):\n",
    "    # compute P(x_j|y=1) and P(x_j|y=0) for j = 1, ..., d\n",
    "    spam_row_index=np.where(y_train == 1)[0]\n",
    "    ham_row_index=np.where(y_train == 0)[0]\n",
    "    \n",
    "    p_on_spam_m=[]\n",
    "    p_on_ham_m=[]\n",
    "    \n",
    "    #num_vocab is d\n",
    "    for i in np.arange(0,num_vocab):\n",
    "        spam_train_frt=x_train_frt[spam_row_index,i]\n",
    "        ham_train_frt=x_train_frt[ham_row_index,i]\n",
    "\n",
    "        p_xi1_y1=(sum(spam_train_frt!=0)+m)/(len(spam_train_frt)+m*num_vocab) #p(xi=1|y=1)\n",
    "        p_on_spam_m.append(p_xi1_y1)\n",
    "        \n",
    "        p_xi1_y0=(sum(ham_train_frt!=0)+m)/(len(ham_train_frt)+m*num_vocab) #p(xi=1|y=0)\n",
    "        p_on_ham_m.append(p_xi1_y0)    \n",
    "\n",
    "    return np.array(p_on_spam_m), np.array(p_on_ham_m)\n",
    "\n",
    "def log_estimated_probability(p_spam, p_ham, p_on_spam_m, p_on_ham_m, x_frts):\n",
    "    # compute log(P(y=1, x_1, x_2,..., x_n)) and log(P(y=0, x_1, x_2,..., x_n))\n",
    "    #log(P(y, x_1, x_2,..., x_n)) = log(P(y=1)) + log(P(x_1 | y)) + ... + log(P(x_d | y))\n",
    "\n",
    "    log_p_spam=[]\n",
    "    log_p_ham=[]\n",
    "    for i in np.arange(0,x_frts.shape[0]):    \n",
    "        spam_index=np.where(x_test_frt[i]!=0)[0]\n",
    "        ham_index=np.where(x_test_frt[i]== 0)[0]\n",
    "    \n",
    "        log_p_spam_i=np.log(p_spam)+sum(np.log(p_on_spam_m[spam_index]))+sum(np.log(1-p_on_spam_m[ham_index]))\n",
    "        log_p_spam.append(log_p_spam_i)\n",
    "    \n",
    "        log_p_ham_i=np.log(p_ham)+sum(np.log(p_on_ham_m[spam_index]))+sum(np.log(1-p_on_ham_m[ham_index]))\n",
    "        log_p_ham.append(log_p_ham_i)\n",
    "    \n",
    "    return np.array(log_p_spam), np.array(log_p_ham)\n",
    "\n",
    "def accuarcy(y_true, y_pred):\n",
    "    # calculate accuracy\n",
    "    y_pred=y_pred*1\n",
    "    result=sum(y_pred==y_true)/len(y_true)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T02:46:56.253606Z",
     "start_time": "2019-09-11T02:46:54.907026Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:05:34.270338Z",
     "start_time": "2019-09-17T02:05:32.712000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 1000) (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# load data\n",
    "train_data, test_data = data_loader()\n",
    "\n",
    "# extract labels to 0,1 and features to dicticnary\n",
    "x_train, y_train, x_test, y_test = data_preprocessing(train_data, test_data)\n",
    "\n",
    "# transform word dicts to feature vectors\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "x_train_frt = vectorizer.fit_transform(x_train)\n",
    "x_test_frt = vectorizer.transform(x_test)\n",
    "#feature vector, columns are features, rows are training/testing data, entry value = number of occurences ith feature in jth record\n",
    "#DictVectorizer will one hot encode categorical value and keep numerical value unchanged\n",
    "\n",
    "print(x_train_frt.shape, x_test_frt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for the best $m$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:05:37.374062Z",
     "start_time": "2019-09-17T02:05:37.364074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-90-d43ed5e80bb7>:40: RuntimeWarning: divide by zero encountered in log\n",
      "  log_p_spam_i=np.log(p_spam)+sum(np.log(p_on_spam_m[spam_index]))+sum(np.log(1-p_on_spam_m[ham_index]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.932\n",
      "0.01:0.927\n",
      "0.1:0.924\n",
      "1:0.922\n",
      "10:0.905\n",
      "100:0.88\n",
      "1000:0.845\n"
     ]
    }
   ],
   "source": [
    "def pipeline(x_train_frt, y_train, x_test_frt, y_test, m):\n",
    "    p_spam, p_ham = compute_empirical_pmf_y(y_train)\n",
    "    p_on_spam_m, p_on_ham_m = m_estimation_conditional_probability(x_train_frt, y_train, x_train_frt.shape[1], m)\n",
    "    log_p_spam, log_p_ham = log_estimated_probability(p_spam, p_ham, p_on_spam_m, p_on_ham_m, x_test_frt)\n",
    "    test_pred = (log_p_spam > log_p_ham)\n",
    "    print(str(m) + \":\" + str(accuarcy(y_test, test_pred)))\n",
    "\n",
    "m_grid = [0,0.01, 0.1, 1, 10, 100, 1000]\n",
    "for m in m_grid:\n",
    "    pipeline(x_train_frt, y_train, x_test_frt, y_test, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vs CountVectorizer\n",
    "# >>> from sklearn.feature_extraction.text import CountVectorizer\n",
    "# >>> corpus = [\n",
    "# ...     'This is the first document.',\n",
    "# ...     'This document is the second document.',\n",
    "# ...     'And this is the third one.',\n",
    "# ...     'Is this the first document?',\n",
    "# ... ]\n",
    "# >>> vectorizer = CountVectorizer()\n",
    "# >>> X = vectorizer.fit_transform(corpus)\n",
    "# >>> vectorizer.get_feature_names_out()\n",
    "# array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "#        'this'], ...)\n",
    "# >>> print(X.toarray())\n",
    "# [[0 1 1 1 0 0 1 0 1]\n",
    "#  [0 2 0 1 0 1 1 0 1]\n",
    "#  [1 0 0 1 1 0 1 1 1]\n",
    "#  [0 1 1 1 0 0 1 0 1]]\n",
    "# >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "# >>> X2 = vectorizer2.fit_transform(corpus)\n",
    "# >>> vectorizer2.get_feature_names_out()\n",
    "# array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "#        'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "#        'this document', 'this is', 'this the'], ...)\n",
    "#  >>> print(X2.toarray())\n",
    "#  [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    "#  [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    "#  [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    "#  [0 0 1 0 1 0 1 0 0 0 0 0 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "90c54abfee7b04b58581da627331e79059a60a43b608b7e15d6f76f7651b8a78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
